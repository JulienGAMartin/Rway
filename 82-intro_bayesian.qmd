# Introduction to Bayesian Inference

## Lecture

Amazing beasties and crazy animals

```{r}
#| echo: false
#| out-width: 50%
#| fig-align: center
#| fig-cap: Dream pet dragon
knitr::include_graphics("images/fun_dragon.jpg")
```


need to add stuff here

### Bayes' theorem

First, let's review the theorem. Mathematically, it says how to convert one conditional probability into another one.

$$ P(B \mid A) = \frac{ P(A \mid B) * P(B)}{P(A)} $$

The formula becomes more interesting in the context of statistical modeling. We
have some model that describes a data-generating process and we have some
*observed* data, but we want to estimate some *unknown* model parameters. 
In that case, the formula reads like:

$$ P(\text{hypothesis} \mid \text{data}) = \frac{ P(\text{data} \mid \text{hypothesis}) * P(\text{hypothesis})}{P(\text{data})} $$

These terms have conventional names:

$$ \text{posterior} = \frac{ \text{likelihood} * \text{prior}}{\text{evidence}} $$

*Prior* and *posterior* describe when information is obtained: what we know pre-data is our
prior information, and what we learn post-data is the updated information
("posterior"). 

The *likelihood* in the equation says how likely the data is given the model
parameters. I think of it as *fit*: How well do the parameters fit the data?
Classical regression's line of best fit is the maximum likelihood line. The
likelihood also encompasses the data-generating process behind the model. For
example, if we assume that the observed data is normally distributed, then we
evaluate the likelihood by using the normal probability density function. You
don't need to know what that last sentence means. What's important is that the
likelihood contains our built-in assumptions about how the data is distributed.

The *evidence* (sometimes called *average likelihood*) is hareder to grasp. I am not sure how to describe it in an intuitive way.
It's there to make sure the math works out so that the posterior probabilities sum to 1.
Some presentations of Bayes' theorem gloss over it and I am not the exception `r emoji::emoji("smile")`.
The important thing to note is that the posterior is proportional to the
likelihood and prior information.

$$ 
\text{posterior information} \propto 
  \text{likelihood of data} * \text{prior information} 
$$

So simply put, **you update your prior information in proportion to how well it fits
the observed data**. So essentially you are doing that on a daily basis for everything except when you ar doing frequentist stats `r emoji::emoji("smile")`.



```{r}
#| eval: false
#| echo: false
#| fig-width: 10.5
#| fig-height: 3.5
#| out-width: 100%
#| fig-cap: Bayesian Triptych

data <- tibble(
  age = c(38, 45, 52, 61, 80, 74),
  prop = c(0.146, 0.241, 0.571, 0.745, 0.843, 0.738)
)

inv_logit <- function(x) 1 / (1 + exp(-x))

model_formula <- bf(
  # Logistic curve
  prop ~ inv_logit(asymlogit) * inv(1 + exp((mid - age) * exp(scale))),
  # Each term in the logistic equation gets a linear model
  asymlogit ~ 1,
  mid ~ 1,
  scale ~ 1,
  # Precision
  phi ~ 1,
  # This is a nonlinear Beta regression model
  nl = TRUE,
  family = Beta(link = identity)
)

prior_fixef <- c(
  # Point of steepest growth is age 4 plus/minus 2 years
  prior(normal(48, 12), nlpar = "mid", coef = "Intercept"),
  prior(normal(1.25, .75), nlpar = "asymlogit", coef = "Intercept"),
  prior(normal(-2, 1), nlpar = "scale", coef = "Intercept")
)

prior_phi <- c(
  prior(normal(2, 1), dpar = "phi", class = "Intercept")
)

fit_prior <- brm(
  model_formula,
  data = data,
  prior = c(prior_fixef, prior_phi),
  iter = 2000,
  chains = 4,
  sample_prior = "only",
  cores = 1,
  control = list(adapt_delta = 0.9, max_treedepth = 15)
)
draws_prior <- data %>%
  tidyr::expand(age = 0:100) %>%
  tidybayes::add_fitted_draws(fit_prior, n = 300)

p1 <- ggplot(draws_prior) +
  aes(x = age, y = .value) +
  geom_line(aes(group = .draw), alpha = .2) +
  theme(
    axis.ticks = element_blank(),
    axis.text = element_blank(),
    axis.title = element_blank()
  ) +
  expand_limits(y = 0:1) +
  ggtitle("Plausible curves before seeing data")

fm1 <- nls(prop ~ SSlogis(age, Asym, xmid, scal), data)
new_data <- tibble(age = 0:100) %>%
  mutate(
    fit = predict(fm1, newdata = .)
  )

point_orange <- "#FB6542"

p2 <- ggplot(data) +
  aes(x = age, y = prop) +
  geom_line(aes(y = fit), data = new_data, size = 1) +
  geom_point(color = point_orange, size = 2) +
  theme(
    axis.ticks = element_blank(),
    axis.text = element_blank(),
    axis.title = element_blank()
  ) +
  expand_limits(y = 0:1) +
  expand_limits(x = c(0, 100)) +
  ggtitle("How well do the curves fit the data")

fit <- brm(
  model_formula,
  data = data,
  prior = c(prior_fixef, prior_phi),
  iter = 2000,
  chains = 4,
  cores = 1,
  control = list(adapt_delta = 0.9, max_treedepth = 15)
)

draws_posterior <- data %>%
  tidyr::expand(age = 0:100) %>%
  tidybayes::add_fitted_draws(fit, n = 100)

p3 <- ggplot(draws_posterior) +
  aes(x = age, y = .value) +
  geom_line(aes(group = .draw), alpha = .2) +
  geom_point(
    aes(y = prop),
    color = point_orange, size = 2,
    data = data
  ) +
  theme(
    axis.ticks = element_blank(),
    axis.text = element_blank(),
    axis.title = element_blank()
  ) +
  expand_limits(y = 0:1) +
  ggtitle("Plausible curves after seeing data")

p1 + p2 + p3
ggsave("bayestriptic.png", width = 10.4, height = 5.1)
```

```{r}
#| echo: false
#| out-width: 100%
#| fig-align: center
#| fig-cap: Bayesian Triptych
knitr::include_graphics("images/bayestriptic.png")
```

::: {.callout-warning}

**A word of encouragement!** The prior is an intimidating part of Bayesian
statistics. It seems highly subjective, as though we are pulling numbers from
thin air, and it can be overwhelming for complex models. But if we are familiar
with the kind of data we are modeling, we have prior information. We can have
the model simulate new observations using the prior distribution and then
plot the hypothetical data. Does anything look wrong or implausible about the
simulated data? If so, then we have some prior information that we can include
in our model. Note that we do not evaluate the plausibility of the simulated
data based on the data we have in hand (the data we want to model); that's not 

:::


### Intro to MCMC

We will now walk through a simple example coded in `R` to illustrate how an MCMC algorithm works.


Suppose you are interested in the mean heart rate is of students when asked a question in a stat course. You are not sure what the exact mean value is, but you know the values are normally distributed with a standard deviation of 15. You have observed 5 individuals to have heart rate of `104, 120,160,90,130`. You could use MCMC sampling to draw samples from the target distribution.
We need to specify:

1. the starting value for the chain. 
2. the length of the chain. In general, more iterations will give you more accurate output.


```{r}
library(coda)
library(bayesplot)
set.seed(170)
hr_obs <- c(104, 112, 132, 115, 110)

start_value <- 250

n_iter <- 2500 # define number of iterations

pd_mean <- numeric(n_iter) # create vector for sample values

pd_mean[1] <- start_value # define starting value

for (i in 2:n_iter) {
  proposal <- pd_mean[i - 1] + MASS::mvrnorm(1, 0, 5) # proposal
  lprop <- sum(dnorm(proposal, hr_obs, 15)) # likelihood of proposed parameter
  lprev <- sum(dnorm(pd_mean[i - 1], hr_obs, 15))
  if (lprop / lprev > runif(1)) { # if likelihood of prosposed > likehood previous accept
    # and if likelihood is lower accept with random noise
    pd_mean[i] <- proposal
  } # if true sample the proposal
  else {
    (pd_mean[i] <- pd_mean[i - 1])
  } # if false sample the current value
}
pd_mean <- as.mcmc(data.frame(mean = pd_mean))
mcmc_combo(pd_mean, combo = c("trace", "dens"))
summary(pd_mean)
```


```{r}
set.seed(170)
hr_obs <- c(104, 112, 132, 115, 110)
n_iter <- 2500 # define number of iterations

n_chain <- 3
start_value <- c(250, 100, 50)

pd_mean <- array(NA, dim = c(n_iter, n_chain, 1), dimnames = list(iter = NULL, chain = NULL, params = "beta")) # create vector for sample values

for (j in seq_len(n_chain)) {
  pd_mean[1, j, 1] <- start_value[j] # define starting value
  for (i in 2:n_iter) {
    proposal <- pd_mean[i - 1, j, 1] + MASS::mvrnorm(1, 0, 5) # proposal
    if (sum(dnorm(proposal, hr_obs, 15)) # likelihood of proposed parameter
    / sum(dnorm(pd_mean[i - 1, j, 1], hr_obs, 15)) > runif(1, 0, 1)) {
      pd_mean[i, j, 1] <- proposal
    } # if true sample the proposal
    else {
      (pd_mean[i, j, 1] <- pd_mean[i - 1, j, 1])
    } # if false sample the current value
  }
}
color_scheme_set("mix-blue-red")
mcmc_combo(pd_mean, combo = c("trace", "dens_overlay"))
summary(pd_mean)

mcmc_combo(pd_mean, combo = c("trace", "dens_overlay"), n_warmup = 500)

pd_burn <- pd_mean[-c(1:500), , , drop = FALSE]
summary(pd_burn)

mcmc_combo(pd_burn, combo = c("trace", "dens_overlay"), iter1 = 501)
```


### Inferences

#### Fixed effects

Easy peazy lemon squeezy just have a look at the posteriro distribution, does it overlap 0 yes or no.

talk about mean, median and mode of a distribution as well as credible intervals

#### Random effects

Quite a bit more harder. because constrained to be positive

- Interpreting posterior distribution
- DIC
- WAIC


## Practical

In this practical, we will revisit our analysis on unicorn aggressivity.
Honestly, we can use any other data with repeated measures for this exercise
but I just love unicorns `r emoji::emoji("heart")`.
However, instead of fittng the model using `lmer()` from the `lmerTest` 
`r emoji::emoji("package")` [@lmerTest], we will refit the model using 2 excellent 
softwares fitting models with a Bayesian approach: `MCMCglmm` [@MCMCglmm] and
`brms` [@brms2021].


### R packages needed

First we load required libraries
```{r}
#| label: loadlibs_bayes
#| message: false
#| results: hide
#| warning: false
library(lmerTest)
library(tidyverse)
library(rptR)
library(brms)
library(MCMCglmm)
library(bayesplot)
```

### A refresher on unicorn ecology

The last model on unicorns was:

```r
aggression ~ opp_size + scale(body_size, center = TRUE, scale = TRUE)
              + scale(assay_rep, scale = FALSE) + block
              + (1 | ID)
```

Those scaled terms are abit a sore for my eyes and way too long if we need to type them multiple times in this practical.
So first let's recode them.
- 
```{r}
unicorns <- read.csv("data/unicorns_aggression.csv")
unicorns <- unicorns %>%
  mutate(
    body_size_sc = scale(body_size),
    assay_rep_sc = scale(assay_rep, scale = FALSE)
  )
```

Ok now we can fit the same model by just using:

```r
aggression ~ opp_size + body_size_sc + assay_rep_sc + block
              + (1 | ID)
```

We can now fit a model using `lmer()`. Since we want to compare a bit `REML` and `Bayesian` aproaches, I am going to wrap the model function in a function called `system.time()`.
This function simply estimate the `user` and `computer` time use by the function.

```{r}
mer_time <- system.time(
  m_mer <- lmer(
    aggression ~ opp_size + body_size_sc + assay_rep_sc + block
      + (1 | ID),
    data = unicorns
  )
)
mer_time
summary(m_mer)
```

Ok so it took no time at all to do it and we got our "classic" results.

### MCMCglmm

What makes `MCMCglmm` so useful and powerful `r emoji::emoji("muscle")` in ecology and for *practical Bayesian people* is that:

1. it is blazing fast `r emoji::emoji("fast")` (for Bayesian analysis) for some models particularly models with structured covariances
2. it is fairly intuitive to code

**but** it also has some inconvenients:

1. it is blazing fast for **Bayesian analysis** meaning it is `r emoji::emoji("snail")` compared to *maximum likelihood* approaches
2. it has some limitations in terms of functionality, distribution availability and model specifications compared to other *Bayesian* softwares
3. the priors, *oh, the priors* `r emoji::emoji("loudly_crying_face")`, are a bit tricky to code and understand `r emoji::emoji("exploding_head")`.


#### Fitting the Model

So here is how we can code the model in `MCMCglmm()`. It is fairly similar to `lmer()` except that the random effects are specified in a different *argument*.

```{r}
#| cache: true
mcglm_time <- system.time(
  m_mcmcglmm <- MCMCglmm(
    aggression ~ opp_size + body_size_sc + assay_rep_sc + block,
    random = ~ID,
    data = unicorns
  )
)
summary(m_mcmcglmm)
mcglm_time
```

Model is slow and not good. We need more iteration and maybe even a longer burnin, and honestly maybe better priors.

We can still take the time to have a look at the R object output from `MCMCglmm()`. The 2 main parts we are interrested in are:

- `Sol` which stand for the model solution and includes the posteriro distribution of the fixed effects
- `VCV`, for the variance covariance estimates, which includes the posterior distribution of all (co)variances estimates for both random effects and residual variance.

```{r}
#| fig.cap: "Posterior trace and distribution of the parameters in m_mcmcglmm using default settings"
#| warning: false
omar <- par()
par(mar = c(4, 2, 1.5, 2))
plot(m_mcmcglmm$Sol)
plot(m_mcmcglmm$VCV)
par(omar)
autocorr.diag(m_mcmcglmm$VCV)
```

Talk about autocorrelation, mixing, convergence and priors here
```{r}
#| cache: true
n_samp <- 1000
thin <- 500
burnin <- 20000
mcglm_time <- system.time(
  m_mcmcglmm <- MCMCglmm(
    aggression ~ opp_size + body_size_sc + assay_rep_sc + block,
    random = ~ID,
    data = unicorns,
    nitt = n_samp * thin + burnin, thin = thin, burnin = burnin,
    verbose = FALSE,
    prior = list(
      R = list(V = 1, nu = 0.002),
      G = list(
        G1 = list(V = 1, nu = 0.002)
      )
    )
  )
)
summary(m_mcmcglmm)
mcglm_time
```
 evaluate model here
```{r}
#| fig.cap: "Posterior trace and distribution of the paremeters in m_mcmcglmm with better settings"
#| warning: false
omar <- par()
par(mar = c(4, 2, 1.5, 2))
plot(m_mcmcglmm$Sol)
plot(m_mcmcglmm$VCV)
par(omar)
autocorr.diag(m_mcmcglmm$VCV)
```

### Inferences

#### Fixed effects

Easy peazy lemon squeezy just have a look at the posterior distribution, does it overlap 0 yes or no.

```{r}
posterior.mode(m_mcmcglmm$Sol)
HPDinterval(m_mcmcglmm$Sol)
```

#### Random effects

Quite a bit more harder. because constrained to be positive

```{r}
posterior.mode(m_mcmcglmm$VCV)
HPDinterval(m_mcmcglmm$VCV)
```

### brms

**brms** is an acronym for *Bayesian Regression Models using 'Stan'* [@brms2021]. It is a package developed to fit regression models with a Bayesian approach using the amazing `stan` software [@stan2021]. 

What makes `brms` so useful and powerful `r emoji::emoji("muscle")` in ecology is that:

1. it is really intuitive to code (same syntax as `glmer()`)
2. it is incredibly flexible since it is essentially a front end for `stan` via its `rstan` interface [@rstan]

**but** with *great powers come great responsability* `r emoji::emoji("spider")`


```{r}
#| cache: true
#| fig-cap: Autocorrelation in the chain for variance parameters in model m_brm
brm_time <- system.time(
  m_brm <- brm(
    aggression ~ opp_size + body_size_sc + assay_rep_sc + block
      + (1 | ID),
    data = unicorns, iter = 4750, warmup = 1000, thin = 15, cores = 4
    # refresh = 0
  )
)
brm_time
summary(m_brm)
mcmc_acf_bar(m_brm, regex_pars = c("sd"))
```

#### Hunder the hood 
have a look at the `stan` code

```{r}
stancode(m_brm)
```

#### using shiny


```{r}
#| eval: false
launch_shinystan(m_brm)
```
```{r}
#| echo: false
#| out-width: 50%
#| fig-align: center
#| fig-cap: Shinystan interface
knitr::include_graphics("images/shinystan.png")
```

### Inferences

#### Fixed effects

```{r}
#| fig-cap: Fixed effect estimates (with 95% credible intervals) from model m_brm
summary(m_brm)
mcmc_plot(m_brm, regex_pars = "b_")
```

#### Random effects

```{r}
#| fig-cap: Among-individual and residual standard deviance ( with 95% credible intervals)
#|   estimated from model m_brm
summary(m_brm)
mcmc_plot(m_brm, pars = c("sd_ID__Intercept", "sigma"))
```


### Happy Bayesian stats

```{r}
#| echo: false
#| out-width: 50%
#| fig-align: center
#| fig-cap: Sherlock Holmes, a truly bayesian detective
knitr::include_graphics("images/sherlock.jpg")
```

